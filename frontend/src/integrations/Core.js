// API åŸºç¡€ URL - ä»ŽçŽ¯å¢ƒå˜é‡è¯»å–
const API_BASE_URL = import.meta.env.VITE_API_BASE_URL || 'http://localhost:3001';

// åœ¨æ‚¨çš„ integrations/Core.js ä¸­æ·»åŠ æµå¼æ”¯æŒ
export const InvokeLLMStream = async ({ prompt, model }) => {
  try {
    const response = await fetch('/api/chat/stream', {
      method: 'POST',
      headers: {
        'Content-Type': 'application/json',
      },
      body: JSON.stringify({
        prompt,
        model,
      }),
    });

    if (!response.ok) {
      throw new Error('Stream request failed');
    }

    return response; // è¿”å›žåŽŸå§‹çš„responseå¯¹è±¡ä¾›å¤„ç†
  } catch (error) {
    console.error('Stream API Error:', error);
    throw error;
  }
};



export const InvokeLLM = async ({ prompt, model = 'general' }) => {
  try {
    console.log(`Calling backend API for model: ${model}`);
    
    const response = await fetch(`${API_BASE_URL}/chat`, {
      method: 'POST',
      headers: {
        'Content-Type': 'application/json',
      },
      body: JSON.stringify({
        prompt,
        model
      })
    });

    // 20251015ä¿®æ”¹è°ƒç”¨log
    console.log('ðŸ“¨ Response Status:', response.status);

    if (!response.ok) {
      const errorText = await response.text();
      console.error('âŒ API Bad Response:', errorText);
      throw new Error(`API Require Fail: ${response.status} ${response.statusText}`);
    }

    const data = await response.json();
    console.log('âœ… API Response Success:', data);
    return data.response;

  } catch (error) {
    console.error('ðŸ”´ API Config Error:', error);
    throw error;
  }
};
// ä¿®æ”¹å®Œæ¯•

// èŽ·å–å¯ç”¨æ¨¡åž‹
export const GetAvailableModels = async () => {
  try {
    const response = await fetch(`${API_BASE_URL}/chat/models`);
    
    if (!response.ok) {
      throw new Error('Failed to fetch available models');
    }
    
    const data = await response.json();
    return data;
  } catch (error) {
    console.error('Error fetching models:', error);
    return { models: ['general'], prompts: {} };
  }
};