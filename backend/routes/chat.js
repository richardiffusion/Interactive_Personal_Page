import express from 'express';
import axios from 'axios';

// 20251016 æ·»åŠ è°ƒå…¥ä»£ç ï¼ˆç§»åŠ¨åˆ°æœ€ä¸Šé¢å¯ä»¥ä½¿ç”¨ï¼‰
import dotenv from 'dotenv';
dotenv.config();

const router = express.Router();

// AI API é…ç½®
const AI_CONFIG = {
  deepseek: {
    url: process.env.DEEPSEEK_API_URL || 'https://api.deepseek.com/chat/completions',
    key: process.env.DEEPSEEK_API_KEY,
    model: process.env.DEEPSEEK_MODEL || 'deepseek-chat'
  },
  openai: {
    url: process.env.OPENAI_API_URL || 'https://api.openai.com/v1/chat/completions',
    key: process.env.OPENAI_API_KEY,
    model: process.env.OPENAI_MODEL || 'gpt-3.5-turbo'
  },
  anthropic: {
    url: process.env.ANTHROPIC_API_URL || 'https://api.anthropic.com/v1/messages',
    key: process.env.ANTHROPIC_API_KEY,
    model: process.env.ANTHROPIC_MODEL || 'claude-3-sonnet-20240229'
  },
  // æ”¯æŒæ‰€æœ‰å‰ç«¯æ¨¡å‹
  general: {
    url: process.env.DEEPSEEK_API_URL || 'https://api.deepseek.com/chat/completions',
    key: process.env.DEEPSEEK_API_KEY,
    model: process.env.DEEPSEEK_MODEL || 'deepseek-chat'
  },
  creative: {
    url: process.env.DEEPSEEK_API_URL || 'https://api.deepseek.com/chat/completions',
    key: process.env.DEEPSEEK_API_KEY,
    model: process.env.DEEPSEEK_MODEL || 'deepseek-chat'
  },
  technical: {
    url: process.env.DEEPSEEK_API_URL || 'https://api.deepseek.com/chat/completions',
    key: process.env.DEEPSEEK_API_KEY,
    model: process.env.DEEPSEEK_MODEL || 'deepseek-chat'
  }
};

// ä»ç¯å¢ƒå˜é‡è¯»å–æ¨¡å‹æç¤ºè¯ï¼Œå¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨é»˜è®¤å€¼
const getModelPrompts = () => {
  // ä»ç¯å¢ƒå˜é‡è¯»å–
  const envPrompts = {
    deepseek: process.env.DEEPSEEK_PROMPT,
    creative: process.env.CREATIVE_PROMPT,
    technical: process.env.TECHNICAL_PROMPT,
    general: process.env.GENERAL_PROMPT,
  };

  // é»˜è®¤æç¤ºè¯ï¼ˆå½“ç¯å¢ƒå˜é‡æ²¡æœ‰è®¾ç½®æ—¶ä½¿ç”¨ï¼‰
  const defaultPrompts = {
    deepseek: "You are Richard Li, an AI Project Development Engineer with 8 years of software development experience. Provide thorough, well-reasoned responses with clear logical steps, drawing from your expertise in AI agents, Python, JavaScript, and full-stack development.",
    creative: "You are Richard Li, a creative AI engineer who combines technical expertise with innovative thinking. Be imaginative and expressive in your responses, particularly when discussing AI agent development, project management, or technology solutions.",
    technical: "You are Richard Li, a technical expert specializing in AI agents, Python, JavaScript, React, and Node.js. Provide clear, practical solutions with code examples when relevant, leveraging your 8 years of software development experience.",
    general: "You are Richard Li, a helpful and professional AI engineer with extensive experience in AI project development and management. Provide balanced, informative responses that reflect your background in software engineering and AI technologies.",
  };

  // åˆå¹¶ï¼šå¦‚æœç¯å¢ƒå˜é‡æœ‰å€¼åˆ™ä½¿ç”¨ï¼Œå¦åˆ™ä½¿ç”¨é»˜è®¤å€¼
  const mergedPrompts = {};
  Object.keys(defaultPrompts).forEach(key => {
    mergedPrompts[key] = envPrompts[key] || defaultPrompts[key];
  });

  return mergedPrompts;
};

// 20251022æ–°å¢ï¼šæµå¼èŠå¤©æ¥å£
router.post('/stream', async (req, res) => {
  // è®¾ç½® SSE (Server-Sent Events) å¤´éƒ¨
  res.writeHead(200, {
    'Content-Type': 'text/event-stream',
    'Cache-Control': 'no-cache',
    'Connection': 'keep-alive',
    'Access-Control-Allow-Origin': '*',
  });

  try {
    const { prompt, model: modelType = 'general' } = req.body;

    if (!prompt) {
      res.write(`data: ${JSON.stringify({ error: 'Prompt is required' })}\n\n`);
      res.end();
      return;
    }

    // æ–°å¢ï¼šè®°å½•ç”¨æˆ·å®é™…æé—®
    console.log('ğŸ‘¤ User Question:', {
      model: modelType,
      question: prompt,
      timestamp: new Date().toISOString(),
      ip: req.ip || req.connection.remoteAddress
    });

    // æ£€æŸ¥æ˜¯å¦å¯ç”¨æ¨¡æ‹Ÿæ¨¡å¼
    const MOCK_MODE = process.env.MOCK_MODE === 'true';
    if (MOCK_MODE) {
      console.log(`ğŸ¤– Mock Stream Mode: Using model ${modelType} to respond`);
      
      const mockResponses = {
        general: `This is a general response to "${prompt}". Currently using the general assistant mode.`,
        creative: `ğŸ¨ Creative mode response to "${prompt}": Let me answer this question in an imaginative way...`,
        technical: `âš™ï¸ Technical mode response to "${prompt}": Analyzing this question from a technical perspective...`,
        deepseek: `ğŸ¤” DeepSeek analysis of "${prompt}": Let me answer this with logical reasoning...`
      };
      
      const response = mockResponses[modelType] || mockResponses.general;
      const words = response.split('');
      
      // æ¨¡æ‹Ÿé€å­—è¾“å‡º
      for (let i = 0; i < words.length; i++) {
        await new Promise(resolve => setTimeout(resolve, 30));
        res.write(`data: ${JSON.stringify({ 
          content: words[i],
          done: false 
        })}\n\n`);
      }
      
      // å‘é€å®Œæˆä¿¡å·
      res.write(`data: ${JSON.stringify({ 
        done: true,
        model: modelType,
        timestamp: new Date().toISOString()
      })}\n\n`);
      res.end();
      return;
    }

    const config = AI_CONFIG[modelType];
    if (!config) {
      res.write(`data: ${JSON.stringify({ error: `Unsupported model: ${modelType}` })}\n\n`);
      res.end();
      return;
    }

    if (!config.key || config.key.includes('your_') || config.key === 'your_deepseek_api_key_here') {
      res.write(`data: ${JSON.stringify({ 
        error: `API key for ${modelType} is not configured`,
        message: 'Please set MOCK_MODE=true or configure API keys in .env file'
      })}\n\n`);
      res.end();
      return;
    }

    // åŠ¨æ€è·å–æç¤ºè¯
    const MODEL_PROMPTS = getModelPrompts();
    const systemPrompt = MODEL_PROMPTS[modelType] || MODEL_PROMPTS.general;
    const fullPrompt = `${systemPrompt}\n\nUser: ${prompt}\n\nAssistant:`;

    console.log(`ğŸ“ Stream Mode: Prompt Used (${modelType}):`, systemPrompt.substring(0, 100) + '...');

    // æµå¼è¯·æ±‚
    const response = await axios.post(config.url, {
      model: config.model,
      messages: [{ role: 'user', content: fullPrompt }],
      stream: true,  // å…³é”®ï¼šå¯ç”¨æµå¼
      temperature: 0.7,
      max_tokens: 2000
    }, {
      headers: {
        'Content-Type': 'application/json',
        'Authorization': `Bearer ${config.key}`
      },
      responseType: 'stream'  // å…³é”®ï¼šæ¥æ”¶æµå¼å“åº”
    });

    let buffer = '';
    
    response.data.on('data', (chunk) => {
      buffer += chunk.toString();
      
      // å¤„ç†æµå¼æ•°æ®
      const lines = buffer.split('\n');
      buffer = lines.pop() || '';
      
      for (const line of lines) {
        if (line.startsWith('data: ') && !line.includes('[DONE]')) {
          try {
            const data = JSON.parse(line.slice(6));
            const content = data.choices?.[0]?.delta?.content || '';
            
            if (content) {
              res.write(`data: ${JSON.stringify({ 
                content: content,
                done: false 
              })}\n\n`);
            }
          } catch (e) {
            // å¿½ç•¥è§£æé”™è¯¯
          }
        }
      }
    });

    response.data.on('end', () => {
      // æ–°å¢ï¼šè®°å½•æµå¼å“åº”å®Œæˆ
      console.log(`âœ… Stream Response Completed for: "${prompt.substring(0, 50)}${prompt.length > 50 ? '...' : ''}"`);
      
      
      // å‘é€å®Œæˆä¿¡å·
      res.write(`data: ${JSON.stringify({ 
        done: true,
        model: modelType,
        timestamp: new Date().toISOString()
      })}\n\n`);
      res.end();
    });

    response.data.on('error', (error) => {
      console.error('Stream Error:', error);
      res.write(`data: ${JSON.stringify({ 
        error: 'Stream connection failed',
        details: error.message 
      })}\n\n`);
      res.end();
    });

  } catch (error) {
    console.error('AI API Stream Error:', error.response?.data || error.message);
    res.write(`data: ${JSON.stringify({ 
      error: 'Failed to get stream response',
      details: error.message 
    })}\n\n`);
    res.end();
  }
});


// éæµå¼èŠå¤©æ¥å£
router.post('/', async (req, res) => {
  try {
    
    // 20251016æµ‹è¯•ä»£ç 
    console.log('ğŸ”§ Receive Chat Request:', req.body);
    console.log('ğŸ”§ MOCK_MODE:', process.env.MOCK_MODE);
    console.log('ğŸ”§ NODE_ENV:', process.env.NODE_ENV);
    
    const { prompt, model: modelType = 'general' } = req.body;

    console.log('ğŸ”§ Request Model:', modelType);
    console.log('ğŸ”§ AI_CONFIG Check:', {
      general: {
        hasKey: !!AI_CONFIG.general.key,
        key: AI_CONFIG.general.key ? 'Configured' : 'not configured',
        keyValue: AI_CONFIG.general.key ? AI_CONFIG.general.key.substring(0, 10) + '...' : 'empty'
      }
    });
    // æµ‹è¯•ä»£ç æ­¢

    if (!prompt) {
      return res.status(400).json({ error: 'Prompt is required' });
    }

    // æ£€æŸ¥æ˜¯å¦å¯ç”¨æ¨¡æ‹Ÿæ¨¡å¼
    const MOCK_MODE = process.env.MOCK_MODE === 'true';
    if (MOCK_MODE) {
      console.log(`ğŸ¤– Mock Mode: Using model ${modelType} to respond`);
      const mockResponses = {
        general: `This is a general response to "${prompt}". Currently using the general assistant mode.`,
        creative: `ğŸ¨ Creative mode response to "${prompt}": Let me answer this question in an imaginative way...`,
        technical: `âš™ï¸ Technical mode response to "${prompt}": Analyzing this question from a technical perspective...`,
        deepseek: `ğŸ¤” DeepSeek analysis of "${prompt}": Let me answer this with logical reasoning...`
      };
      
      // æ¨¡æ‹Ÿå¤„ç†æ—¶é—´
      await new Promise(resolve => setTimeout(resolve, 1000));
      
      return res.json({ 
        response: mockResponses[modelType] || mockResponses.general,
        model: modelType,
        timestamp: new Date().toISOString(),
        mock: true
      });
    }

    const config = AI_CONFIG[modelType];
    if (!config) {
      return res.status(400).json({ error: `Unsupported model: ${modelType}` });
    }

    // 20251016ï¼šä¿®æ”¹è¿™éƒ¨åˆ†ä»£ç  - æ›´å®½æ¾çš„æ£€æŸ¥
    if (!config.key || config.key.includes('your_') || config.key === 'your_deepseek_api_key_here') {
      return res.status(500).json({ 
        error: `API key for ${modelType} is not configured`,
        message: 'Please set MOCK_MODE=true or configure API keys in .env file'
      });
    }

    // åŠ¨æ€è·å–æç¤ºè¯
    const MODEL_PROMPTS = getModelPrompts();
    const systemPrompt = MODEL_PROMPTS[modelType] || MODEL_PROMPTS.general;
    const fullPrompt = `${systemPrompt}\n\nUser: ${prompt}\n\nAssistant:`;

    console.log(`ğŸ“ Using prompt (${modelType}):`, systemPrompt.substring(0, 100) + '...');

    let response;
    
    if (modelType === 'anthropic') {
      // Anthropic API æ ¼å¼
      response = await axios.post(config.url, {
        model: config.model,
        max_tokens: 4000,
        messages: [{ role: 'user', content: fullPrompt }]
      }, {
        headers: {
          'Content-Type': 'application/json',
          'x-api-key': config.key,
          'anthropic-version': '2023-06-01'
        }
      });
    } else {
      // OpenAI å…¼å®¹æ ¼å¼ (DeepSeek, OpenAI)
      response = await axios.post(config.url, {
        model: config.model,
        messages: [{ role: 'user', content: fullPrompt }],
        stream: false
      }, {
        headers: {
          'Content-Type': 'application/json',
          'Authorization': `Bearer ${config.key}`
        }
      });
    }

    let aiResponse;
    
    if (modelType === 'anthropic') {
      aiResponse = response.data.content[0].text;
    } else {
      aiResponse = response.data.choices[0].message.content;
    }

    res.json({ 
      response: aiResponse,
      model: modelType,
      timestamp: new Date().toISOString()
    });

  } catch (error) {
    console.error('AI API Error:', error.response?.data || error.message);
    
    const statusCode = error.response?.status || 500;
    const errorMessage = error.response?.data?.error?.message || 'Failed to get response from AI service';
    
    res.status(statusCode).json({ 
      error: errorMessage,
      details: process.env.NODE_ENV === 'production' ? undefined : error.response?.data
    });
  }
});


// 20251016ä¿®æ”¹
// è·å–å¯ç”¨æ¨¡å‹
router.get('/models', (req, res) => {
  const MOCK_MODE = process.env.MOCK_MODE === 'true';
  
  // åœ¨æ¨¡æ‹Ÿæ¨¡å¼ä¸‹ï¼Œæ‰€æœ‰æ¨¡å‹éƒ½å¯ç”¨
  const availableModels = MOCK_MODE 
    ? Object.keys(AI_CONFIG)
    : Object.keys(AI_CONFIG).filter(model => 
        AI_CONFIG[model].key && AI_CONFIG[model].key !== 'your_deepseek_api_key_here'
      );
  
  // è·å–å½“å‰ä½¿ç”¨çš„æç¤ºè¯
  const MODEL_PROMPTS = getModelPrompts();
  
  res.json({ 
    models: availableModels,
    prompts: MODEL_PROMPTS,
    mockMode: MOCK_MODE
  });
});

// // è·å–å¯ç”¨æ¨¡å‹å’Œå½“å‰æç¤ºè¯
// router.get('/models', (req, res) => {
//   const availableModels = Object.keys(AI_CONFIG).filter(model => 
//     AI_CONFIG[model].key && AI_CONFIG[model].key !== 'your_api_key_here'
//   );
  
//   // è·å–å½“å‰ä½¿ç”¨çš„æç¤ºè¯
//   const MODEL_PROMPTS = getModelPrompts();
  
//   res.json({ 
//     models: availableModels,
//     prompts: MODEL_PROMPTS
//   });
// });


// æ·»åŠ ç¯å¢ƒå˜é‡æ£€æŸ¥
console.log('ğŸ”§ Environment Variable Check:');
console.log('MOCK_MODE:', process.env.MOCK_MODE);
console.log('DEEPSEEK_API_KEY:', process.env.DEEPSEEK_API_KEY ? 'Configured' : 'Not Configured');
console.log('NODE_ENV:', process.env.NODE_ENV);



export default router;